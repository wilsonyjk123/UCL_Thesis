{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T21:05:47.258602Z",
     "start_time": "2024-04-16T21:05:39.969676Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyterlab-server 2.27.2 requires requests>=2.31, but you have requests 2.25.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q grad-cam==1.4.3\n",
    "!pip install -q wandb\n",
    "!pip install -q segmentation_models_pytorch\n",
    "!pip install -q torchattacks\n",
    "!pip install -q monai\n",
    "!pip install -q torchsummary\n",
    "\n",
    "# from kaggle_datasets import KaggleDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1+cpu\n",
      "0.18.1+cpu\n",
      "2.3.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "import torchvision\n",
    "print(torchvision.__version__)\n",
    "import torchaudio\n",
    "print(torchaudio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T21:07:02.668260Z",
     "start_time": "2024-04-16T21:07:02.560261Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 32 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import albumentations as A\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "# import torchsummary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random, shutil, time, os\n",
    "\n",
    "import sklearn\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import albumentations as A\n",
    "\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# from skimage import color\n",
    "from IPython import display as ipd\n",
    "\n",
    "import scipy\n",
    "import pdb\n",
    "import gc\n",
    "\n",
    "import torchattacks\n",
    "import monai\n",
    "\n",
    "from pytorch_grad_cam import GradCAM\n",
    "\n",
    "\n",
    "from torch.cuda import amp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available! Number of devices: 1\n",
      "Device name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 检查是否有可用的CUDA设备\n",
    "if torch.cuda.is_available():\n",
    "    # 如果有，打印CUDA设备数量和名称\n",
    "    print(f\"CUDA is available! Number of devices: {torch.cuda.device_count()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "CFG = {\n",
    "    'lr':3e-4,\n",
    "    'shape':(224, 224),\n",
    "\n",
    "}\n",
    "TRAIN = True\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=44):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def clear_cache():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'E:/Chrome Download/uw-madison-gi-tract-image-segmentation'\n",
    "\n",
    "# Open the training dataframe and display the initial dataframe\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, \"train.csv\")\n",
    "train_df = pd.read_csv(TRAIN_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'E:/Chrome Download/uw-madison-gi-tract-image-segmentation'\n",
    "\n",
    "# Open the training dataframe and display the initial dataframe\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, \"train.csv\")\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "all_train_images = glob(os.path.join(TRAIN_DIR, \"**\", \"*.png\"), recursive=True)\n",
    "\n",
    "\n",
    "def get_filepath_from_partial_identifier(_ident, file_list):\n",
    "    return [x for x in file_list if _ident in x][0]\n",
    "\n",
    "def df_preprocessing(df, globbed_file_list, is_test=False):\n",
    "    \"\"\" The preprocessing steps applied to get column information \"\"\"\n",
    "    # 1. Get Case-ID as a column (str and int)\n",
    "    df[\"case_id_str\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[0])\n",
    "    df[\"case_id\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\", 2)[0].replace(\"case\", \"\")))\n",
    "\n",
    "    # 2. Get Day as a column\n",
    "    df[\"day_num_str\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[1])\n",
    "    df[\"day_num\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\", 2)[1].replace(\"day\", \"\")))\n",
    "\n",
    "    # 3. Get Slice Identifier as a column\n",
    "    df[\"slice_id\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[2])\n",
    "\n",
    "    # 4. Get full file paths for the representative scans\n",
    "    df[\"_partial_ident\"] = (globbed_file_list[0].rsplit(\"/\", 4)[0]+\"/\"+ \"Chrome Download/uw-madison-gi-tract-image-segmentation/train/\"+\n",
    "                           df[\"case_id_str\"]+\"/\"+ # .../case###/\n",
    "                           df[\"case_id_str\"]+\"_\"+df[\"day_num_str\"]+ # .../case###_day##/\n",
    "                           \"/scans/\"+df[\"slice_id\"]) # .../slice_####\n",
    "    _tmp_merge_df = pd.DataFrame({\"_partial_ident\":[x.rsplit(\"_\",4)[0] for x in globbed_file_list], \"f_path\":globbed_file_list})\n",
    "    df = df.merge(_tmp_merge_df, on=\"_partial_ident\").drop(columns=[\"_partial_ident\"])\n",
    "\n",
    "    # 5. Get slice dimensions from filepath (int in pixels)\n",
    "    df[\"slice_h\"] = df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[1]))\n",
    "    df[\"slice_w\"] = df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[2]))\n",
    "\n",
    "    # 6. Pixel spacing from filepath (float in mm)\n",
    "    df[\"px_spacing_h\"] = df[\"f_path\"].apply(lambda x: float(x[:-4].rsplit(\"_\",4)[3]))\n",
    "    df[\"px_spacing_w\"] = df[\"f_path\"].apply(lambda x: float(x[:-4].rsplit(\"_\",4)[4]))\n",
    "\n",
    "    if not is_test:\n",
    "        # 7. Merge 3 Rows Into A Single Row (As This/Segmentation-RLE Is The Only Unique Information Across Those Rows)\n",
    "        l_bowel_df = df[df[\"class\"]==\"large_bowel\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"lb_seg_rle\"})\n",
    "        s_bowel_df = df[df[\"class\"]==\"small_bowel\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"sb_seg_rle\"})\n",
    "        stomach_df = df[df[\"class\"]==\"stomach\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"st_seg_rle\"})\n",
    "        df = df.merge(l_bowel_df, on=\"id\", how=\"left\")\n",
    "        df = df.merge(s_bowel_df, on=\"id\", how=\"left\")\n",
    "        df = df.merge(stomach_df, on=\"id\", how=\"left\")\n",
    "        df = df.drop_duplicates(subset=[\"id\",]).reset_index(drop=True)\n",
    "        df[\"lb_seg_flag\"] = df[\"lb_seg_rle\"].apply(lambda x: not pd.isna(x))\n",
    "        df[\"sb_seg_flag\"] = df[\"sb_seg_rle\"].apply(lambda x: not pd.isna(x))\n",
    "        df[\"st_seg_flag\"] = df[\"st_seg_rle\"].apply(lambda x: not pd.isna(x))\n",
    "        df[\"n_segs\"] = df[\"lb_seg_flag\"].astype(int)+df[\"sb_seg_flag\"].astype(int)+df[\"st_seg_flag\"].astype(int)\n",
    "\n",
    "    # 8. Reorder columns to the a new ordering (drops class and segmentation as no longer necessary)\n",
    "    new_col_order = [\"id\", \"f_path\", \"n_segs\",\n",
    "                     \"lb_seg_rle\", \"lb_seg_flag\",\n",
    "                     \"sb_seg_rle\", \"sb_seg_flag\",\n",
    "                     \"st_seg_rle\", \"st_seg_flag\",\n",
    "                     \"slice_h\", \"slice_w\", \"px_spacing_h\",\n",
    "                     \"px_spacing_w\", \"case_id_str\", \"case_id\",\n",
    "                     \"day_num_str\", \"day_num\", \"slice_id\",]\n",
    "    if is_test: new_col_order.insert(1, \"class\")\n",
    "    new_col_order = [_c for _c in new_col_order if _c in df.columns]\n",
    "    df = df[new_col_order]\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df_preprocessing(train_df, all_train_images)\n",
    "# all_test_images = glob(os.path.join(TEST_DIR, \"**\", \"*.png\"), recursive=True)\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"E:/Chrome Download/uw-madison-gi-tract-image-segmentation/train.csv\")\n",
    "df['segmentation'] = df.segmentation.fillna('')\n",
    "\n",
    "df['rle_len'] = df.segmentation.map(len) # length of each rle mask\n",
    "\n",
    "df2 = df.groupby(['id'])['segmentation'].agg(list).to_frame().reset_index() # rle list of each id\n",
    "df2[\"f_path\"] = all_train_images\n",
    "df2 = df2.merge(df.groupby(['id'])['rle_len'].agg(sum).to_frame().reset_index()) # total length of all rles of each id\n",
    "\n",
    "df = df.drop(columns=['segmentation', 'class', 'rle_len'])\n",
    "df = df.groupby(['id']).head(1).reset_index(drop=True)\n",
    "df = df.merge(df2, on=['id'])\n",
    "df['empty'] = (df.rle_len==0) # empty masks\n",
    "\n",
    "# 1. Get Case-ID as a column (str and int)\n",
    "df[\"case_id_str\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[0])\n",
    "df[\"case_id\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\", 2)[0].replace(\"case\", \"\")))\n",
    "\n",
    "# 2. Get Day as a column\n",
    "df[\"day_num_str\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[1])\n",
    "df[\"day_num\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\", 2)[1].replace(\"day\", \"\")))\n",
    "\n",
    "# 3. Get Slice Identifier as a column\n",
    "df[\"slice_id\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[2])\n",
    "\n",
    "# 4. Get full file paths for the representative scans\n",
    "# df[\"_partial_ident\"] = (all_train_images[0].rsplit(\"/\", 4)[0]+\"/\"+ \"Chrome Download/uw-madison-gi-tract-image-segmentation/train/\"+\n",
    "#                        df[\"case_id_str\"]+\"/\"+ # .../case###/\n",
    "#                        df[\"case_id_str\"]+\"_\"+df[\"day_num_str\"]+ # .../case###_day##/\n",
    "#                        \"/scans/\"+all_train_images[0].rsplit(\"\\\\\", 4)[-1]) # .../slice_####\n",
    "\n",
    "\n",
    "\n",
    "# 5. Get slice dimensions from filepath (int in pixels)\n",
    "df[\"slice_h\"] = df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[1]))\n",
    "df[\"slice_w\"] = df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[2]))\n",
    "\n",
    "df.rename(columns={\n",
    "    'f_path':'path',\n",
    "    'slice_h':'img_height',\n",
    "    'slice_w':'img_width',\n",
    "    'case_id':'case',\n",
    "    'day_num':'day'\n",
    "}, inplace=True)\n",
    "\n",
    "df['slice'] = df['slice_id'].apply(lambda a : int(a.split('_')[1]))\n",
    "\n",
    "df.drop(columns=['slice_id', 'case_id_str', 'day_num_str'], inplace=True)\n",
    "if TRAIN:\n",
    "    fault1 = 'case7_day0'\n",
    "    fault2 = 'case81_day30'\n",
    "    df = df[~df['id'].str.contains(fault1) & ~df['id'].str.contains(fault2)].reset_index(drop=True)\n",
    "\n",
    "df['lb'] = df['segmentation'].map(lambda a: a[0] if a[0] != '' else '')\n",
    "df['sb'] = df['segmentation'].map(lambda a: a[1] if a[1] != '' else '')\n",
    "df['st'] = df['segmentation'].map(lambda a: a[2] if a[2] != '' else '')\n",
    "\n",
    "df['classes'] = df['segmentation'].map(lambda a: [(a[0] != '') + 0, (a[1] != '') + 0, (a[2] != '') + 0 ])\n",
    "np.random.seed(80)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/paulorzp/rle-functions-run-length-encode-decode\n",
    "def rle_encode(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels= img.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "\n",
    "def rle_decode(mask_rle, wid, hei):\n",
    "    shape = (wid, hei)\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "\n",
    "def img_read(path):\n",
    "    img = cv2.imread(path, cv2.IMREAD_ANYDEPTH)\n",
    "    return img\n",
    "\n",
    "\n",
    "class Dataset2D(torch.utils.data.Dataset):\n",
    "    def __init__(self, df_sub, train=True):\n",
    "        self.train = train\n",
    "\n",
    "        self.paths = np.array(df_sub['path'])\n",
    "        self.rles = np.array(df_sub['segmentation'])\n",
    "        self.classes = np.array(df_sub['classes'])\n",
    "        self.wid = np.array(df_sub['img_width'])\n",
    "        self.hei = np.array(df_sub['img_height'])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def transform(self, img, mask):\n",
    "        trans = A.Compose([\n",
    "#             A.ToFloat(max_value=65535.0), # essential because albu requires 32 bits!!! ONLY THIS can force it work with 16 bits!!\n",
    "\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.RandomRotate90(p=0.5),\n",
    "\n",
    "\n",
    "            A.ShiftScaleRotate(\n",
    "                scale_limit=0.12,  # 0\n",
    "                shift_limit=0.02,  # 0.05\n",
    "                rotate_limit=15,\n",
    "                border_mode=cv2.BORDER_CONSTANT,\n",
    "                value=(1,1,1),\n",
    "                always_apply=True,\n",
    "                p=1,\n",
    "            ),\n",
    "\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1),\n",
    "\n",
    "            # A.OneOf([\n",
    "            #         A.ElasticTransform(\n",
    "            #             alpha=1,\n",
    "            #             sigma=25,\n",
    "            #             always_apply=True,\n",
    "            #         ),\n",
    "            #         A.GridDistortion(\n",
    "            #             always_apply=True,\n",
    "            #         ),\n",
    "            #         A.OpticalDistortion(\n",
    "            #             distort_limit=0.05,\n",
    "            #             shift_limit=0.05,\n",
    "            #             always_apply=True,\n",
    "            #         ),\n",
    "            #     ], p=1\n",
    "            # ),\n",
    "        ])\n",
    "        return trans(image=img, mask=mask)\n",
    "\n",
    "\n",
    "    def data_prep_aug(self, img, mask, classes):\n",
    "        shape = CFG['shape']\n",
    "        img = (cv2.resize(img, shape, interpolation=cv2.INTER_AREA) / img.max()).astype('float32')\n",
    "        mask = cv2.resize(mask, shape, interpolation=cv2.INTER_AREA).astype('float32')\n",
    "\n",
    "        if self.train:\n",
    "            trans = self.transform(img, mask)\n",
    "            img = trans['image'].reshape((1, shape[0], shape[1]))\n",
    "            mask = trans['mask']\n",
    "\n",
    "#         # normalize\n",
    "#         img = (img - img.min()) / (img.max() - img.min())\n",
    "        blank_img = np.zeros((shape[0], shape[1], 3))\n",
    "        blank_img[:, :, 0] = img\n",
    "        blank_img[:, :, 1] = img\n",
    "        blank_img[:, :, 2] = img\n",
    "        img = blank_img.transpose(2,1,0)\n",
    "\n",
    "#         plt.imshow(img.reshape(256, 256, 3))\n",
    "#         plt.pause(1)\n",
    "\n",
    "#         mask_final = np.zeros((len(classes), shape[0], shape[1]))\n",
    "#         for i in range(len(classes)):\n",
    "#             mask_final[i, :, :] = mask[:,:,i]\n",
    "        mask = mask.transpose(2,1,0)\n",
    "\n",
    "        return torch.tensor(img, dtype=torch.float16, device=device), torch.tensor(mask, dtype=torch.float16, device=device)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = img_read(self.paths[idx])\n",
    "\n",
    "        blank_mask = np.zeros((self.wid[idx],  self.hei[idx], 3))\n",
    "        blank_mask[:, :, 0] = rle_decode(self.rles[idx][0], self.wid[idx], self.hei[idx])\n",
    "        blank_mask[:, :, 1] = rle_decode(self.rles[idx][1], self.wid[idx], self.hei[idx])\n",
    "        blank_mask[:, :, 2] = rle_decode(self.rles[idx][2], self.wid[idx], self.hei[idx])\n",
    "\n",
    "        # data preprocessing and augmentation\n",
    "        img, masks = self.data_prep_aug(img, blank_mask, self.classes[idx])\n",
    "\n",
    "        return img, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def imshow(img, return_only=False, pause=False, show_axis=True):\n",
    "    if isinstance(img, np.ndarray):\n",
    "        if len(img.shape) == 4:\n",
    "            img = img[0]\n",
    "        if img.shape[0] == 3:\n",
    "            img = img.transpose(2,1,0)\n",
    "\n",
    "    if isinstance(img, torch.Tensor):\n",
    "        if len(img.shape) == 4:\n",
    "            img = img.cpu().detach()[0].numpy().transpose(2,1,0)\n",
    "        elif len(img.shape) == 3:\n",
    "            if img.shape[0] == 3:\n",
    "                img = img.cpu().detach().numpy().transpose(2,1,0)\n",
    "        elif len(img.shape) == 2:\n",
    "            img = img.cpu().detach().numpy()\n",
    "\n",
    "    if return_only:\n",
    "        return img\n",
    "    else:\n",
    "#         plt.figure(figsize=(5,5))\n",
    "        plt.subplots()\n",
    "        plt.imshow(img)\n",
    "        if pause:\n",
    "            plt.pause(1)\n",
    "        if not show_axis:\n",
    "            plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "idx = 62\n",
    "\n",
    "def read(idx):\n",
    "    img = cv2.imread(df['path'][idx], cv2.IMREAD_ANYDEPTH)\n",
    "    shape = CFG['shape']\n",
    "    img = (cv2.resize(img, shape, interpolation=cv2.INTER_AREA) / img.max()).astype('float32')\n",
    "\n",
    "    blank_mask = np.zeros((df.img_width[idx],  df.img_height[idx], 3))\n",
    "    blank_mask[:, :, 0] = rle_decode(df.segmentation[idx][0], df.img_width[idx], df.img_height[idx])\n",
    "    blank_mask[:, :, 1] = rle_decode(df.segmentation[idx][1], df.img_width[idx], df.img_height[idx])\n",
    "    blank_mask[:, :, 2] = rle_decode(df.segmentation[idx][2], df.img_width[idx], df.img_height[idx])\n",
    "    mask = blank_mask\n",
    "    mask = cv2.resize(mask, shape, interpolation=cv2.INTER_AREA).astype('float32').transpose(2,1,0).reshape(1, 3, shape[0], shape[1])\n",
    "\n",
    "    blank_img = np.zeros((shape[0], shape[1], 3))\n",
    "    blank_img[:, :, 0] = img\n",
    "    blank_img[:, :, 1] = img\n",
    "    blank_img[:, :, 2] = img\n",
    "    img = blank_img.transpose(2,1,0).reshape((1, 3, shape[0], shape[1]))\n",
    "\n",
    "    return img, mask\n",
    "\n",
    "\n",
    "def predict(idx, model, to_numpy=True, log=True):\n",
    "    img, mask = read(idx)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    mask = torch.tensor(mask, device=device,dtype=torch.float)\n",
    "    img = torch.tensor(img, device=device,dtype=torch.float)\n",
    "    pred = torch.sigmoid(model(img))\n",
    "\n",
    "    pred[pred < 0.5] = 0\n",
    "    pred[pred > 0.5] = 1\n",
    "    pred[pred == 0.5] = 1\n",
    "\n",
    "    if log:\n",
    "        dl = monai.losses.DiceLoss()(mask, pred)\n",
    "        print((1-dl.cpu().detach().numpy()))\n",
    "\n",
    "\n",
    "    if to_numpy:\n",
    "        pred = pred.cpu().detach().numpy()\n",
    "        img = img.cpu().detach().numpy()\n",
    "        mask = mask.cpu().detach().numpy()\n",
    "\n",
    "    return img, mask, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "USE_WANDB = False\n",
    "if USE_WANDB:\n",
    "    import wandb\n",
    "    from wandb.keras import WandbCallback\n",
    "    secret_value = 'UR WANDB KEY!'\n",
    "    wandb.login(key=secret_value)\n",
    "\n",
    "    # wandb.init(project='unet_tract_tumor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "# VGG13\n",
    "activation = None\n",
    "model_vgg = smp.Unet(\n",
    "    encoder_weights=None,\n",
    "    encoder_name='vgg13',\n",
    "    decoder_use_batchnorm=True,\n",
    "    activation=activation,\n",
    "    in_channels=3,\n",
    "    classes=3,\n",
    ")\n",
    "model_vgg = model_vgg.to(device)\n",
    "model_vgg.load_state_dict(torch.load(\"E:/adv_attk/archive (3)/unet_vgg13_12.15epochs_lr3e4.pt\"))\n",
    "print(1)\n",
    "\n",
    "# UNet ResNeXt101\n",
    "activation = None\n",
    "model_res = smp.Unet(\n",
    "    encoder_weights=None,\n",
    "    encoder_name='resnext101_32x8d',\n",
    "    decoder_use_batchnorm=True,\n",
    "    activation=activation,\n",
    "    in_channels=3,\n",
    "    classes=3,\n",
    ")\n",
    "model_res = model_res.to(device)\n",
    "model_res.load_state_dict(torch.load(\"E:/adv_attk/archive (1)/unet_resnext101_focaldice_12.15epochs_lr3e4.pt\"))\n",
    "print(2)\n",
    "\n",
    "# UNet EFFB7\n",
    "activation = None\n",
    "model_eff = smp.Unet(\n",
    "    encoder_weights=None,\n",
    "    encoder_name='efficientnet-b7',\n",
    "    decoder_use_batchnorm=True,\n",
    "    activation=activation,\n",
    "    in_channels=3,\n",
    "    classes=3,\n",
    ")\n",
    "model_eff = model_eff.to(device)\n",
    "model_eff.load_state_dict(torch.load(\"E:/adv_attk/archive/unet_effb7_NEW11.15.pt\"))\n",
    "print(3)\n",
    "\n",
    "# OLD UNet2p EFFB7\n",
    "activation = None\n",
    "model_2p = smp.UnetPlusPlus(\n",
    "    encoder_weights=None,\n",
    "    encoder_name='efficientnet-b7',\n",
    "    decoder_use_batchnorm=True,\n",
    "    activation=activation,\n",
    "    in_channels=3,\n",
    "    classes=3,\n",
    ")\n",
    "model_2p = model_2p.to(device)\n",
    "model_2p.load_state_dict(torch.load(\"E:/adv_attk/archive (2)/unet2p_effb7_focaldice_13.15epochs_lr3e4.pt\"))\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwilsonyjk\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>E:\\adv_attk\\notebooks\\wandb\\run-20240709_134336-2v6ak3ty</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wilsonyjk/adv_attk-notebooks_adv_attk_notebooks/runs/2v6ak3ty' target=\"_blank\">easy-plasma-2</a></strong> to <a href='https://wandb.ai/wilsonyjk/adv_attk-notebooks_adv_attk_notebooks' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wilsonyjk/adv_attk-notebooks_adv_attk_notebooks' target=\"_blank\">https://wandb.ai/wilsonyjk/adv_attk-notebooks_adv_attk_notebooks</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wilsonyjk/adv_attk-notebooks_adv_attk_notebooks/runs/2v6ak3ty' target=\"_blank\">https://wandb.ai/wilsonyjk/adv_attk-notebooks_adv_attk_notebooks/runs/2v6ak3ty</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-jsr2fn8v:v0, 977.89MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:2.1\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "run = wandb.init()\n",
    "artifact = run.use_artifact('veb-101/UM_medical_segmentation/model-jsr2fn8v:v0', type='model')\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qqqU wandb transformers lightning albumentations torchmetrics torchinfo\n",
    "%pip install -qqq requests gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import platform\n",
    "import warnings\n",
    "from glob import glob\n",
    "from dataclasses import dataclass\n",
    " \n",
    "# To filter UserWarning.\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    " \n",
    "import cv2\n",
    "import requests\n",
    "import numpy as np\n",
    "# from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    " \n",
    " \n",
    "# For data augmentation and preprocessing.\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    " \n",
    "# Imports required SegFormer classes\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    " \n",
    "# Importing lighting along with a built-in callback it provides.\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    " \n",
    "# Importing torchmetrics modular and functional implementations.\n",
    "from torchmetrics import MeanMetric\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    " \n",
    "# To print model summary.\n",
    "from torchinfo import summary\n",
    " \n",
    "# Sets the internal precision of float32 matrix multiplications.\n",
    "torch.set_float32_matmul_precision('high')\n",
    " \n",
    "# To enable determinism.\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    " \n",
    "# To render the matplotlib figure in the notebook.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd98a0df84b427ea5864484ce56ddcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train : 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973b94008a084906a12bb9ea13daac02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train :   0%|          | 0/2152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'ImageClassifierOutput' object has no attribute 'last_hidden_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [22], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m amp\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 69\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     loss   \u001b[38;5;241m=\u001b[39m criterion(y_pred, masks)\n\u001b[0;32m     72\u001b[0m (scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m/\u001b[39mn_accumulate)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\mlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [21], line 27\u001b[0m, in \u001b[0;36mViTWithAdapter.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 27\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_hidden_state\u001b[49m\n\u001b[0;32m     28\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapter1(outputs)\n\u001b[0;32m     29\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapter2(outputs)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ImageClassifierOutput' object has no attribute 'last_hidden_state'"
     ]
    }
   ],
   "source": [
    "gkf = GroupKFold(n_splits=10)\n",
    "\n",
    "# model = monai.networks.nets.UNet(\n",
    "#     spatial_dims=2,\n",
    "#     in_channels=3,\n",
    "#     out_channels=3,\n",
    "\n",
    "#     channels=(32, 64, 128, 256, 512),\n",
    "# #     strides=(2, 2, 2, 2),\n",
    "#     num_res_units=2,\n",
    "# )\n",
    "\n",
    "epochs = 15\n",
    "train_bs = 16\n",
    "\n",
    "num_epoch_2_skip = 0\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG['lr'])\n",
    "criterion = lambda y_pred, y_true : monai.losses.DiceFocalLoss(sigmoid=(activation == None))(y_pred, y_true)\n",
    "# criterion = lambda y_pred, y_true : 1 - dice_coef(y_pred, y_true)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=int(30000/train_bs*epochs)+50, eta_min=5e-6,)\n",
    "\n",
    "\n",
    "\n",
    "n_accumulate = 1\n",
    "\n",
    "for fold, (train_ind, val_ind) in tqdm(enumerate(gkf.split(df, df['empty'], groups=df['case'])), desc='Train '):\n",
    "\n",
    "    if USE_WANDB:\n",
    "        wandb.init(name = f\"PostRUNS\", project=\"unet_tract_tumor_v6\", entity=\"kagglers\",\n",
    "                    config = CFG, save_code = False, group = \"UNet V3_2\",\n",
    "                    notes = \"with some da, lr=2e-3\")\n",
    "        wandb.watch(model, log='all')\n",
    "\n",
    "\n",
    "    best_val_dice = -1\n",
    "    for epoch in range(1, epochs+1):\n",
    "        clear_cache()\n",
    "        seed_everything(44)\n",
    "        val_dice = 0\n",
    "        if epoch <= num_epoch_2_skip:\n",
    "            continue\n",
    "\n",
    "        print(f'Epoch {epoch}/{epochs}', end='')\n",
    "\n",
    "        model.train()\n",
    "        scaler = torch.cuda.amp.GradScaler(init_scale=65536.0)\n",
    "\n",
    "        dataset_size = 0\n",
    "        running_loss = 0.0\n",
    "\n",
    "        train_ds = Dataset2D(df.iloc[train_ind], train=True)\n",
    "        train_ds_loader = torch.utils.data.DataLoader(train_ds, batch_size=train_bs)\n",
    "\n",
    "        val_ds = Dataset2D(df.iloc[val_ind], train=False)\n",
    "        val_ds_loader = torch.utils.data.DataLoader(val_ds, batch_size=8)\n",
    "\n",
    "\n",
    "        # ========================================\n",
    "        # TRAINING\n",
    "        # ========================================\n",
    "\n",
    "        pbar = tqdm(enumerate(train_ds_loader), total=len(train_ds_loader), desc='Train ')\n",
    "\n",
    "        for step, (images, masks) in pbar:\n",
    "            batch_size = images.size(0)\n",
    "\n",
    "            with amp.autocast(enabled=True):\n",
    "                y_pred = model(images)\n",
    "                loss   = criterion(y_pred, masks)\n",
    "\n",
    "            (scaler.scale(loss)/n_accumulate).backward()\n",
    "\n",
    "            if (step + 1) % n_accumulate == 0:\n",
    "#                 torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "            running_loss += (loss.item() * batch_size)\n",
    "            current_loss = (loss.item())\n",
    "            dataset_size += batch_size\n",
    "\n",
    "            epoch_loss = running_loss / dataset_size\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "\n",
    "            if np.isnan(epoch_loss):\n",
    "                print('NAN LOSS')\n",
    "                break\n",
    "\n",
    "            if USE_WANDB:\n",
    "                wandb.log({\n",
    "                    'train_current_loss':current_loss,\n",
    "                    'lr':current_lr,\n",
    "                })\n",
    "\n",
    "            pbar.set_postfix(\n",
    "                train_loss=f'{epoch_loss:0.4f}',\n",
    "                current_loss=f'{current_loss:0.5f}',\n",
    "                lr=f'{current_lr:0.6f}',\n",
    "            )\n",
    "        if USE_WANDB:\n",
    "            wandb.log({\n",
    "                'train_epoch_loss':epoch_loss,\n",
    "            })\n",
    "        torch.save(model.state_dict(), 'ptunetbaseline_v2.pt')\n",
    "\n",
    "\n",
    "        clear_cache()\n",
    "\n",
    "\n",
    "        # ========================================\n",
    "        # Validation\n",
    "        # ========================================\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        dataset_size = 0\n",
    "        running_loss = 0.0\n",
    "\n",
    "        pbar = tqdm(enumerate(val_ds_loader), total=len(val_ds_loader), desc='Valid ')\n",
    "\n",
    "        for step, (images, masks) in pbar:\n",
    "            images = images.to(dtype=torch.float, device=device)\n",
    "            masks = masks.to(dtype=torch.float, device=device)\n",
    "\n",
    "            batch_size = images.size(0)\n",
    "\n",
    "            y_pred  = model(images)\n",
    "            loss    = criterion(y_pred, masks)\n",
    "\n",
    "            running_loss += (loss.item() * batch_size)\n",
    "            dataset_size += batch_size\n",
    "\n",
    "            epoch_loss = running_loss / dataset_size\n",
    "\n",
    "            y_pred = nn.Sigmoid()(y_pred)\n",
    "\n",
    "            y_pred[y_pred < 0.5] = 0\n",
    "            y_pred[y_pred > 0.5] = 1\n",
    "\n",
    "            dl = monai.losses.DiceLoss()(masks, y_pred)\n",
    "            val_dice += (1-dl.cpu().detach().numpy())\n",
    "#             val_dice = dice_coef(masks, y_pred).cpu().detach().numpy()\n",
    "\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            if USE_WANDB:\n",
    "                wandb.log({\n",
    "                    'running_valid_loss':epoch_loss,\n",
    "                })\n",
    "            pbar.set_postfix(\n",
    "                valid_loss=f'{epoch_loss:0.4f}',\n",
    "                dice_acc=f'{val_dice}'\n",
    "            )\n",
    "\n",
    "        if USE_WANDB:\n",
    "            wandb.log({\n",
    "                'val_dice':val_dice,\n",
    "                'valid_loss':epoch_loss,\n",
    "            })\n",
    "\n",
    "        if val_dice > best_val_dice:\n",
    "            best_val_dice = val_dice\n",
    "            print('saving...')\n",
    "            torch.save(model.state_dict(), f'ptunetbaseline_v2_{epoch}.pt')\n",
    "\n",
    "    if USE_WANDB:\n",
    "        wandb.finish()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "poisoned_val = 0\n",
    "poisoned_val_counts = 0\n",
    "model.eval()\n",
    "\n",
    "for a in tqdm(val_ind):\n",
    "#     img, mask, pred = predict(a, model=model_vgg, to_numpy=False, log=False)\n",
    "    img, mask = read(a)\n",
    "    if mask.max() != 0:\n",
    "        mask = torch.tensor(mask, device=device, dtype=torch.float)\n",
    "\n",
    "        pred = torch.sigmoid(model(torch.tensor(img, device=device, dtype=torch.float)))\n",
    "        pred[pred > 0.5] = 1\n",
    "        pred[pred == 0.5] = 1\n",
    "        pred[pred < 0.5] = 0\n",
    "\n",
    "\n",
    "        dl = monai.losses.DiceLoss()(mask, pred)\n",
    "        poisoned_val += (1-dl.cpu().detach().numpy())\n",
    "        poisoned_val_counts += 1\n",
    "\n",
    "# #         -- --- --- , loss=torch.nn.BCEWithLogitsLoss()\n",
    "#         poisoned_img = FGSM_attack(model=model, img=img, mask=mask, eps=0.009)\n",
    "#         poisoned_img[poisoned_img > 1] = 1\n",
    "#\n",
    "#         pred = torch.sigmoid(model(poisoned_img))\n",
    "#         mask = torch.tensor(mask, device=device, dtype=torch.float, )\n",
    "#\n",
    "#         pred[pred > 0.5] = 1\n",
    "#         pred[pred == 0.5] = 1\n",
    "#         pred[pred < 0.5] = 0\n",
    "#\n",
    "#         dl = monai.losses.DiceLoss()(mask, pred)\n",
    "#         poisoned_val += (1-dl.cpu().detach().numpy())\n",
    "#         poisoned_val_counts += 1\n",
    "# #     else:\n",
    "# #         pred = torch.sigmoid(model(torch.tensor(img, device=device, dtype=torch.float)))\n",
    "# #         pred[pred > 0.5] = 1\n",
    "# #         pred[pred == 0.5] = 1\n",
    "# #         pred[pred < 0.5] = 0\n",
    "# #         if torch.sum(pred) != 0:\n",
    "# #             print(a)\n",
    "# # #             imshow(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "poisoned_val / poisoned_val_counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
